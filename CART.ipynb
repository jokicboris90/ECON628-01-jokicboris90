{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cart (Classification and Regression Trees) and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tRecursively partitioning the input space, and defining a local model in each resulting region of input space.\n",
    "\n",
    "•\tTo make a prediction for a given observation, the method typically use the mean or the mode of the training observations in the region to which it belongs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative approach is to sub-divide or **partition** the space into smaller regions, and then partition the subdivision again **(recursive partitioning** process) until we get to chunks of the space that are easier to control so that we can ultimately fit simple models to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms\n",
    "\n",
    "Nodes: points on which decisions will be made\n",
    "\n",
    "Edges: Pathways\n",
    "\n",
    "### Structure\n",
    "\n",
    "root node: Start\n",
    "\n",
    "internal node: Tests conditions \n",
    "\n",
    "leaf node: Final output, means of the observations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the terminal nodes, or leaves, of the tree represents a cell of the partition, and has attached to it a simple model which applies in that cell only.\n",
    "\n",
    "* Then, the model for the leaf is the the sample mean of the dependent variable in that cell:\n",
    "$$\\hat y = \\frac1 c \\sum_{i=1}^c y_i $$ \n",
    "\n",
    "Once the local models are completely determined, we need to find a good tree (this means finding a good partitioning of the data)\n",
    "\n",
    "We need to do a greedy search => find the binary question that maximizes the information we get about  Y , this will create the root node and 2 daughters nodes. Then at each daughter node we repeat this procedure asking which question would give us the maximum information about  Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping criterion\n",
    "\n",
    "* This means stop growing the tree when further splits gives less than some minimal amount of extra information.\n",
    "\n",
    "The sum of squared errors for a tree $T$ is:\n",
    "$$S = \\sum_{c \\in leaves (T)}  \\sum_{i \\in C} (y_i - m_c)^2  $$ \n",
    "* where the prediction for leave $c$ is: \n",
    "$$m_c = \\frac 1 n_c \\sum_{c \\in leaves C} y_i $$ \n",
    "* therefore we can rewrite $S$ as: \n",
    "$$S = \\sum_{c \\in leaves (T)} n_c V_C  $$ \n",
    "* where $V_c$ is the within leave variance of leaf $c$, and now we can make our splits to minimize $S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree pruning\n",
    "\n",
    "* In the minimization process we are more likely to create good predictions on the training set, but there is a high risk of overfitting the data => leads to a poor test performance.\n",
    "* To avoid this we need select a subtree that leads to a lower error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons\n",
    "\n",
    "| **Pros**   |      **Cons**     | \n",
    "|:----------|:-------------:|\n",
    "| Simple to understand/interpret | Overfitting | \n",
    "| Little data preparation (normalization, create dummies) | Greedy algorithm |\n",
    "|Easily handle mixed discrete and continuous inputs|Unstable: small changes to the input data=>large effects on the structure of the tree|\n",
    "|Insensitive to monotone transformations of the input|Trees are high **variance** estimators|\n",
    "|Perform automatic variable selection|\n",
    "|Relatively robust to outliers|\n",
    "|Robust - performs well when assumptions are violated|  \n",
    "|Performs well in large datasets |    \n",
    "|Extremely fast execution | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "* One way to reduce variance of an estimate is to average together many estimates.\n",
    "* We can train $M$ different trees on different subsets of the data, **choosing randomly with replacement** and then compute the **ensemble:**\n",
    "$$ f(X)= \\sum_{m=1}^M \\frac 1M f_m(X)$$\n",
    "\n",
    "where $f_m$ is the $m$’th tree. \n",
    "* This technique is called **bagging** => “bootstrap aggregating”\n",
    "\n",
    "* Re-running the same learning algorithm on different subsets of the data can result in highly correlated predictors.\n",
    "* Random forests tries to decorrelate the base learners by learning trees based on a **randomly** chosen subset of input variables, as well as a **randomly** chosen subset of data cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "* Defining test and train samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define y\n",
    "y = df['y']\n",
    "\n",
    "## Define X (exclude inc, incsq, log_inc)\n",
    "columns_ = df.columns.tolist()\n",
    "exclude_cols = ['x1', 'x2', 'x3']\n",
    "\n",
    "X = df[[i for i in columns_ if i not in exclude_cols]]\n",
    "\n",
    "## Print shapes of y and X\n",
    "print y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Train test split 70/30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "## Print shapes of X(s) and y(s)\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1\n",
    "\n",
    "Build decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "## Here is the gridsearch\n",
    "params = {\"max_depth\": [3,5,10,20],\n",
    "          \"max_features\": [None, \"auto\"],\n",
    "          \"min_samples_leaf\": [1, 3, 5, 7, 10],\n",
    "          \"min_samples_split\": [2, 5, 7],\n",
    "           \"criterion\" : ['mse']\n",
    "         }\n",
    "\n",
    "# ## Here crossvalidate \n",
    "from sklearn.grid_search import GridSearchCV\n",
    "dtr_gs = GridSearchCV(dtr, params, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Fit your tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtr_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Print best stimator, best parameters, and best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' dtr_best = is the regression tree regressor with best parameters/estimators'''\n",
    "dtr_best = dtr_gs.best_estimator_ \n",
    "\n",
    "print \"best estimator\", dtr_best\n",
    "print \"\\n==========\\n\"\n",
    "print \"best parameters\",  dtr_gs.best_params_\n",
    "print \"\\n==========\\n\"\n",
    "print \"best score\", dtr_gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Print feature importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Here I am defining a function to print feature importance using best models'''\n",
    "def feature_importance(X, best_model):\n",
    "    feature_importance = pd.DataFrame({'feature':X.columns, 'importance':best_model.feature_importances_})\n",
    "    feature_importance.sort_values('importance', ascending=False, inplace=True)\n",
    "    return feature_importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importance(X, dtr_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Predict\n",
    "\n",
    "(in the test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_dtr= dtr_best.predict(X_test)\n",
    "y_pred_dtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Evaluate the performance of you model\n",
    "\n",
    "*MSE in train and test data, R2 in train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Function that calls the MSE and R^2 at once, using the name of the method and calling the best model'''\n",
    "\n",
    "def rsquare_meansquare_error(train_y, test_y, train_X, test_X, test, best_model):\n",
    "    \"\"\" first we need to predict on the test and train data\"\"\"\n",
    "    y_train_pred = best_model.predict(train_X)\n",
    "    y_test_pred = best_model.predict(test_X)\n",
    "    \n",
    "    \"\"\" We call the MSE in the following lines\"\"\"\n",
    "    print ('MSE ' + test + ' train data: %.2f, test data: %.2f' % (\n",
    "        mean_squared_error(train_y, y_train_pred),\n",
    "        mean_squared_error(test_y, y_test_pred)))\n",
    "    \n",
    "    \"\"\" We call the R^2 in the following lines\"\"\"\n",
    "    print('R^2 ' + test + ' train data: %.2f, test data: %.2f' % (\n",
    "        r2_score(train_y, y_train_pred),\n",
    "        r2_score(test_y, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rsquare_meansquare_error(y_train, y_test, X_train, X_test, \"Regression tree\", dtr_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Visualize your tree using the best parameters/estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# REQUIREMENTS:\n",
    "# pip install pydotplus\n",
    "# brew install graphviz\n",
    "\n",
    "# Use graphviz to make a chart of the regression tree decision points:\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "#### import pydotplus\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "''' dtr_best was defined before in section B'''\n",
    "\n",
    "## Graph\n",
    "export_graphviz(dtr_best, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,\n",
    "                feature_names=X.columns)  \n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest = RandomForestRegressor( )\n",
    "\n",
    "params = {'max_depth':[3,4,5], \n",
    "          'max_features':[2,3,4], \n",
    "          'max_leaf_nodes':[5,6,7], \n",
    "          'min_samples_split':[3,4],\n",
    "         'n_estimators': [100]\n",
    "         }\n",
    "\n",
    "estimator_rfr = GridSearchCV(forest, params, n_jobs=-1,  cv=5,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Fit random tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator_rfr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Print best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' rfr_best = is the random forest regression tree regressor with best parameters/estimators'''\n",
    "rfr_best = estimator_rfr.best_estimator_\n",
    "print \"best estimator\", rfr_best\n",
    "print \"\\n==========\\n\"\n",
    "print \"best parameters\", estimator_rfr.best_params_\n",
    "print \"\\n==========\\n\"\n",
    "print \"best score\", estimator_rfr.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Print feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importance(X, rfr_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_rfdtr= rfr_best.predict(X_test)\n",
    "y_pred_rfdtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Evaluate the performance of your model \n",
    "* MSE in train and test data, R2 in train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quare_meansquare_error(y_train, y_test, X_train, X_test, \"Random Forest Regression tree\", rfr_best)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
