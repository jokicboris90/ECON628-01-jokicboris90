{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGRESSION TREE METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested to know not just which general features or variables predict the model, but specific predictors of Y (such if male or if young, or if income less than 10000$), then regression tree is appropriate.\n",
    "\n",
    "It involves stratifying or segmenting the predictor space into several simple regions. \n",
    "To make a prediction for a given observation, the method typically use the mean or the mode of the training observations in the region to which it belongs.\n",
    "\n",
    "* Idea: when the data has numerous features that interact in complicated nonlinear, it's difficult to create a one global model.\n",
    "We then need a stratified model, with specific predictors.\n",
    "Node= point/leaf where the decision is made\n",
    "\n",
    "Two models: Regression Tree and Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIMPLE REGRESSION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Import Packages and read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "#http://python.6.x6.nabble.com/IPython-User-ipython-notebook-how-to-display-image-not-from-pylab-td4497427.html\n",
    "\n",
    "# plotting modules\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "# make sure charts appear in the notebook:\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df='~\\'\n",
    "df= pd.read_csv(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Define Y and Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define y\n",
    "y = data['log_inc']\n",
    "\n",
    "## Define X (exclude inc, incsq, log_inc)\n",
    "columns_ = data.columns.tolist()\n",
    "exclude_cols = ['inc', 'incsq', 'log_inc'] \n",
    "X = data[[i for i in columns_ if i not in exclude_cols]] \n",
    "## Print shapes of y and X\n",
    "print y.shape, X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3 : Split Train and Test Samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Train test split 70/30\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "## Print shapes of X(s) and y(s)\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4 : Call and Build a regression Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build a regression tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "#First, we specify the parameters we want in a dictionary list with strings and values. \n",
    "params = {\"max_depth\": [3,5,10,20], \n",
    "          \"max_features\": [None, \"auto\"], \n",
    "          \"min_samples_leaf\": [1, 3, 5, 7, 10],\n",
    "          \"min_samples_split\": [2, 5, 7],\n",
    "           \"criterion\": ['mse']\n",
    "         }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* max_depth specifies how deep we want to search to go into the data. For us, the range is 3 - 20\n",
    "With a max_depth of 1, the model suffers from high bias.\n",
    "With a max_depth of 10, the model suffers from high variance. \n",
    "\n",
    "* Max features, defines the maximum number of independent variables we want to have.\n",
    "none will overfit the model because we will have too much\n",
    "\n",
    "* min_samples_leaf : The minimum number of samples required to be at a leaf node.\n",
    "* 'max_leaf_nodes': The max number of leaves in the tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Cross Validate the method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Here crossvalidate using the Gridsearch\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "dtr_gs = GridSearchCV(dtr, params, n_jobs=-1, cv=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* njobs=-1 removes the criteria (one sample) after its already used.\n",
    "* cv is cross validation and means the sample is split equally, trained, and then tested on 5 different samples.\n",
    "* verbosity specifies the number of message the search will display. Higher verbosity means that as the search goes on, it prints more message about it.\n",
    "\n",
    "* Here is the gridsearch, and we are asking the computer to go and find the one that gives the best model. grid search is a technique to find good values for model parameters that cannot be optimized directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 6 : Fit your best model found in gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit the tree model : Now we need to bring everything together and build a model on the train data\n",
    "dtr_gs.fit(X_train, y_train)\n",
    "\n",
    "#Print best estimator, best parameters, and best score (best fit to explain Y)\n",
    "''' dtr_best = is the regression tree regressor with best parameters/estimators'''\n",
    "\n",
    "dtr_best = dtr_gs.best_estimator_ \n",
    "print \"best estimator\", dtr_best\n",
    "print \"\\n==========\\n\"\n",
    "print \"best parameters\",  dtr_gs.best_params_\n",
    "print \"\\n==========\\n\"\n",
    "print \"best score\", dtr_gs.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7: Define a function that we will use to print all features (indpdt variables) by importance.\n",
    "The ones with most explanatory power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Define Function to Print Feature importances\n",
    "''' Here I am defining a function to print feature importance using best models'''\n",
    "\n",
    "def feature_importance(X, best_model):\n",
    "    feature_importance = pd.DataFrame({'feature':X.columns, 'importance':best_model.feature_importances_})\n",
    "    feature_importance.sort_values('importance', ascending=False, inplace=True)\n",
    "    return feature_importance\n",
    "\n",
    "#Using the function\n",
    "feature_importance(X, dtr_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 8 : Predict on the Test sample. We train a model on the train data part, we need to test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict on the Test Data\n",
    "y_pred_dtr= dtr_best.predict(X_test)\n",
    "y_pred_dtr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 9 : Evaluate the performance of your model (MSE in train and test data, R2 in train and test data)\n",
    "You need to know if your model performed well on the test data. \n",
    "Evaluation using MSE and R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#Define Function that calls the MSE and R^2 at once, using the name of the method and calling the best model\n",
    "\n",
    "def rsquare_meansquare_error(train_y, test_y, train_X, test_X, test, best_model):\n",
    "    \"\"\" first we need to predict on the test and train data\"\"\"\n",
    "    y_train_pred = best_model.predict(train_X)\n",
    "    y_test_pred = best_model.predict(test_X)\n",
    "    \n",
    "    \"\"\" We call the MSE in the following lines\"\"\"\n",
    "    print ('MSE ' + test + ' train data: %.2f, test data: %.2f' % (\n",
    "        mean_squared_error(train_y, y_train_pred),\n",
    "        mean_squared_error(test_y, y_test_pred)))\n",
    "    \n",
    "    \"\"\" We call the R^2 in the following lines\"\"\"\n",
    "    print('R^2 ' + test + ' train data: %.2f, test data: %.2f' % (\n",
    "        r2_score(train_y, y_train_pred),\n",
    "        r2_score(test_y, y_test_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using function\n",
    "rsquare_meansquare_error(y_train, y_test, X_train, X_test, \"Regression tree\", dtr_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the MSE and R^2 results on train and test, and comparing to another model such as random forest, you can tell if your model is good or not. Train and test Samples results have to be very close for a good model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize your tree USING the \"best\" parameteres/estimators\n",
    "# REQUIREMENTS:\n",
    "# pip install pydotplus\n",
    "# brew install graphviz\n",
    "\n",
    "# Use graphviz to make a chart of the regression tree decision points:\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import pydot\n",
    "\n",
    "dot_data = StringIO()\n",
    "''' dtr_best was defined before in section B'''\n",
    "\n",
    "## Graph\n",
    "export_graphviz(dtr_best, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,\n",
    "                feature_names=X.columns)  \n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree will print by descending order of importance the most important features of your model. The most important or the one with most explanatory power will be at the top, and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's called Bagging or “Bootstrap Aggregating”. Instead of training our whole sample, it trains M different trees on different subsets of the data, choosing randomly with replacement and then compute the ensemble. The data are chosen by selecting a random subset of features, and a random subset of observations to train model.\n",
    "Random forests often have very good predictive accuracy, and reduces variance.\n",
    "\n",
    "For commands, same process as before, but just few changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Import Packages and read dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Define Y and Xs (Same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define y\n",
    "y = data['log_inc']\n",
    "\n",
    "## Define X (exclude inc, incsq, log_inc)\n",
    "columns_ = data.columns.tolist()\n",
    "exclude_cols = ['inc', 'incsq', 'log_inc'] \n",
    "X = data[[i for i in columns_ if i not in exclude_cols]] \n",
    "## Print shapes of y and X\n",
    "print y.shape, X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3 : Split Train and Test Samples (Same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Train test split 70/30\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "## Print shapes of X(s) and y(s)\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4 : Call and Build a random forest model (different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build a Random regression tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor( )\n",
    "\n",
    "params = {'max_depth':[3,4,5], \n",
    "          'max_features':[2,3,4], \n",
    "          'max_leaf_nodes':[5,6,7], \n",
    "          'min_samples_split':[3,4],\n",
    "         'n_estimators': [100]\n",
    "         }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Cross Validate the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Here crossvalidate using the Gridsearch\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "estimator_rfr = GridSearchCV(forest, params, n_jobs=-1,  cv=5,verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 6 : Fit your best model found in gridsearch (different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit the tree model : Now we need to bring everything together and build a model on the train data\n",
    "#Print best estimator, best parameters, and best score (best fit to explain Y)\n",
    "\n",
    "''' rfr_best = is the random forest regression tree regressor with best parameters/estimators'''\n",
    "\n",
    "rfr_best = estimator_rfr.best_estimator_\n",
    "print \"best estimator\", rfr_best\n",
    "print \"\\n==========\\n\"\n",
    "print \"best parameters\", estimator_rfr.best_params_\n",
    "print \"\\n==========\\n\"\n",
    "print \"best score\", estimator_rfr.best_score_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7: Define a function that we will use to print all features (indpdt variables) by importance.\n",
    "Just call the function defined before, and print the features\n",
    "The ones with most explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using the function defined in Simple Reg Tree to Print feature Importance\n",
    "feature_importance(X, rfr_best)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 8 : Predict on the Test sample. We train a model on the train data part, we need to test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict on the Test Data\n",
    "y_pred_rfdtr= rfr_best.predict(X_test)\n",
    "y_pred_rfdtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 9 : Evaluate the performance of your model (MSE in train and test data, R2 in train and test data)\n",
    "You need to know if your model performed well on the test data. \n",
    "Evaluation using MSE and R^2.\n",
    "\n",
    "We already created the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate the performance of your model (MSE in train and test data, R2 in train and test data) using function created above\n",
    "rsquare_meansquare_error(y_train, y_test, X_train, X_test, \"Random Forest Regression tree\", rfr_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Step is to compare MSE and R^2 of Simple Regression tree and Random Forest Tree.\n",
    "Choose the one that has the most close values on train and test, and also high R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
